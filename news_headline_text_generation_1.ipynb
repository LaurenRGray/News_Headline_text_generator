{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News headline text generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I create a model that generates news headlines using the Kaggle dataset of New York Times comments and headlines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# necessary imports\n",
    "\n",
    "# keras module for building LSTM \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "\n",
    "# set seeds for reproducability\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(2)\n",
    "seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load the [dataset](https://www.kaggle.com/aashita/nyt-comments) of New York Times comments and headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method from Kaggle kernel\n",
    "\n",
    "curr_dir = '/Users/laurenfinkelstein/take_home_tests/SynapseFI/data/'\n",
    "all_headlines = []\n",
    "for filename in os.listdir(curr_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(curr_dir + filename)\n",
    "        all_headlines.extend(list(article_df.headline.values))\n",
    "        break\n",
    "\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# William method? using glob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean the data\n",
    "\n",
    "- remove punctuation\n",
    "- lowercase all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finding an expansive view  of a forgotten people in niger',\n",
       " 'and now  the dreaded trump curse',\n",
       " 'venezuelas descent into dictatorship',\n",
       " 'stain permeates basketball blue blood',\n",
       " 'taking things for granted',\n",
       " 'the caged beast awakens',\n",
       " 'an everunfolding story',\n",
       " 'oreilly thrives as settlements add up',\n",
       " 'mouse infestation',\n",
       " 'divide in gop now threatens trump tax plan']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower() # removes punctuation and lowercases words\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore') # converts from utf8 to ascii -- REMOVE THIS!!\n",
    "    return txt \n",
    "\n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate sequence of n-gram tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modelling takes in sequential data (i.e., words/tokens), and uses this sequential data to predict the next word/token. This requires tokenization, which is the process of extracting tokens (i.e., terms/words) from a corpus. To do this, I use the Python library Keras' built-in model for tokenization, which is able to obtain the tokens and their respsective index in the corpus. This allows for every text document in the dataset to be converted into a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[169, 17],\n",
       " [169, 17, 665],\n",
       " [169, 17, 665, 367],\n",
       " [169, 17, 665, 367, 4],\n",
       " [169, 17, 665, 367, 4, 2],\n",
       " [169, 17, 665, 367, 4, 2, 666],\n",
       " [169, 17, 665, 367, 4, 2, 666, 170],\n",
       " [169, 17, 665, 367, 4, 2, 666, 170, 5],\n",
       " [169, 17, 665, 367, 4, 2, 666, 170, 5, 667],\n",
       " [6, 80]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus) # fit the tokenizer on the documents\n",
    "    total_words = len(tokenizer.word_index) + 1 # word index gives a dictionary of words and their uniquely assigned integers\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = [] # sequence of n-grams from all documents in the corpus\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0] # list of tokens corresponding to each word in the document (i.e., news headline)\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence) # sequence of n-grams (from 2 word n-grams, through n-grams = len(line))\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each list in the above output represents the ngram phrases generated from the input data (i.e., the documents/headlines). Each integer in these n-grams is the index of the given word in the complete vocabulary of words present in the corpus of text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the first headline in the corpus is \"Finding an expansive view of a forgotten people in niger\". For this headline, we see the following output, i.e. **sequences of tokens**:\n",
    "\n",
    "    [169, 17],\n",
    "    [169, 17, 665],\n",
    "    [169, 17, 665, 367],\n",
    "    [169, 17, 665, 367, 4],\n",
    "    [169, 17, 665, 367, 4, 2],\n",
    "    [169, 17, 665, 367, 4, 2, 666],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170, 5],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170, 5, 667]\n",
    "\n",
    "which, respectively, equate to the following **n-grams**: \n",
    "\n",
    "    Finding an,\n",
    "    Finding an expansive,\n",
    "    Finding an expansive view,\n",
    "    Finding an expansive view of,\n",
    "    Finding an expansive view of a,\n",
    "    Finding an expansive view of a forgotten,\n",
    "    Finding an expansive view of a forgotten people,\n",
    "    Finding an expansive view of a forgotten people in,\n",
    "    Finding an expansive view of a forgotten people in niger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pad the sequences and obtain variables (predictor and target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we've generated a data-set containing a sequence of tokens. It's important to recognize that different sequences may have different lengths. Before training the model, we must ensure all sequence lengths are equal. For this, we can pad the sequences using the Keras pad_sequence function. \n",
    "\n",
    "Additionally, inorder to input data into the model, we must create predictors and a label. We will use a n-grams sequence as predictors, and the next word of the n=gram as the label. \n",
    "\n",
    "For example, for the headline \"Finding an expansive view of a forgotten people in niger\", the **predictors** will be:\n",
    "\n",
    "    Finding an\n",
    "    Finding an expansive\n",
    "    Finding an expansive view\n",
    "\n",
    "and the corresponding **labels** will be\n",
    "\n",
    "    expansive\n",
    "    view\n",
    "    of\n",
    "\n",
    "... and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    ## pad the sequences\n",
    "    max_sequence_len = max([len(x) for x in input_sequences]) # find length of the longest input sequence (i.e., headline)\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')) # pads the input sequences and converts to a numpy array\n",
    "    ## ^ WHY IS IT WRAPPED AS NP.ARRAY? WHEN I TEST PRINT IT WITH/WITHOUT NP.ARRAY(), OUTPUT IS THE SAME\n",
    "    \n",
    "    ## create predictors and label\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1] # label is the last word of each n-gram, and predictors are all the preceeding words\n",
    "    label = ku.to_categorical(label, num_classes=total_words) # one-hot encodes the labels; this is a matrix of 4,806 rows (the number of documents/headlines in the corpus) and 2,422 columns (the number of words/tokens in the corpus (i.e., length of input_sequences))\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can obtain the input vector *X* and the label vector _Y_, which will be used for to training the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use a LSTM model with a three layer architecture, including:\n",
    "    \n",
    "***Input Layer*** : Takes the sequence of words as input\n",
    "<br /> ***LSTM Layer*** : Computes the output using LSTM units. There are currently 100 units, but this can be fine-tuned later.\n",
    "<br /> ***Dropout Layer*** : A regularization layer which randomly turns off the activations of some neurons in the LSTM layer in order to prevent overfitting. (Note: this is an optional layer)\n",
    "<br /> ***Output Layer*** : Computes the probability of the best possible next word as output\n",
    "    \n",
    "We will run this model for  100 epochs, but this can be further experimented with and fine-tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/laurenfinkelstein/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/laurenfinkelstein/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 10)            24220     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2422)              244622    \n",
      "=================================================================\n",
      "Total params: 313,242\n",
      "Trainable params: 313,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## create the model\n",
    "\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1 # - 1 because the last word in the sequence is used as the label (?)\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the model architecture, we can train it using our data (predictors and labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/laurenfinkelstein/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2043f2b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train the model\n",
    "\n",
    "model.fit(predictors, label, epochs=100, verbose=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been trained, we can use it to predict the next word based on input words (i.e., seed text). \n",
    "\n",
    "To feed this seed text into the model, we will need to do some pre-processing, which includes (1) tokenizing the seed text and (2) padding the sequences. We can then pass the sequences into the trained model to get the predicted word. The multiple predicted words can then be appended together to obtain a predicted sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] # list of tokens corresponding to each word in the document (i.e., news headline)\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # add padding\n",
    "        predicted = model.predict_classes(token_list, verbose=0) # make predictions using trained model\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items(): # word index gives a dictionary of words and their uniquely assigned integers\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States Governor Bands Is Youths Is\n",
      "Preident Trump Officials Brace For Anger\n",
      "Donald Trump Country Shock On Trump\n",
      "India And China Deep Flavor That Nafta\n",
      "New York Today Sharing A Block\n",
      "Science And Technology Shut From A Clash Jet\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"united states\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"preident trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"donald trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"new york\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"science and technology\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement Ideas\n",
    "As we can see, the model has produced the output which looks fairly fine. The results can be improved further with following points:\n",
    "\n",
    "- Adding more data\n",
    "- Fine Tuning the network architecture\n",
    "- Fine Tuning the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
