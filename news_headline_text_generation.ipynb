{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News headline text generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I use a LSTM neural network to create a generative model for text, which in this case is news headlines. LSTM recurrent neural networks are commonly used for text generation becuase they are able to learn the sequences of a given problem domain, and then generate entirely new and plausible sequences for that domain. \n",
    "\n",
    "I first develop a simple LSTM network, trained on a Kaggle dataset and data retrieved using the New York Times API, that will learn sequences of words for text generation. I then use this model to generate new sequences of words (i.e., headlines). Because LSTM networks can be slow to train, I've trained the networks in this notebook using an AWS GPU instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# MISC data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# keras modules for preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.utils as ku \n",
    "# keras modules for building LSTM \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "# keras modules for LSTM training\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# set seeds for reproducability\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(2)\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load data\n",
    "\n",
    "### 1. Load the [kaggle dataset](https://www.kaggle.com/aashita/nyt-comments) of New York Times comments and headlines\n",
    "The NYT dataset downloaded from Kaggle contains nine months of data, from the months January - May 2017 and January - April 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781</td>\n",
       "      <td>By JOHN BRANCH</td>\n",
       "      <td>article</td>\n",
       "      <td>Former N.F.L. Cheerleaders’ Settlement Offer: ...</td>\n",
       "      <td>['Workplace Hazards and Violations', 'Football...</td>\n",
       "      <td>68</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:16:49</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>“I understand that they could meet with us, pa...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/sports/foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5adf653f068401528a2aa697</td>\n",
       "      <td>656</td>\n",
       "      <td>By LISA FRIEDMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>E.P.A. to Unveil a New Rule. Its Effect: Less ...</td>\n",
       "      <td>['Environmental Protection Agency', 'Pruitt, S...</td>\n",
       "      <td>68</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:11:21</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>The agency plans to publish a new regulation T...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/climate/epa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5adf4626068401528a2aa628</td>\n",
       "      <td>2427</td>\n",
       "      <td>By PETE WELLS</td>\n",
       "      <td>article</td>\n",
       "      <td>The New Noma, Explained</td>\n",
       "      <td>['Restaurants', 'Noma (Copenhagen, Restaurant)...</td>\n",
       "      <td>66</td>\n",
       "      <td>Dining</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:58:44</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>What’s it like to eat at the second incarnatio...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/dining/noma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5adf40d2068401528a2aa619</td>\n",
       "      <td>626</td>\n",
       "      <td>By JULIE HIRSCHFELD DAVIS and PETER BAKER</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Macron, Emmanuel (1977- )', 'Trump, Donald J...</td>\n",
       "      <td>68</td>\n",
       "      <td>Washington</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:35:57</td>\n",
       "      <td>Europe</td>\n",
       "      <td>President Trump welcomed President Emmanuel Ma...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5adf3d64068401528a2aa60f</td>\n",
       "      <td>815</td>\n",
       "      <td>By IAN AUSTEN and DAN BILEFSKY</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Toronto, Ontario, Attack (April, 2018)', 'Mu...</td>\n",
       "      <td>68</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:21:21</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Alek Minassian, 25, a resident of Toronto’s Ri...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/canad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID  articleWordCount  \\\n",
       "0  5adf6684068401528a2aa69b               781   \n",
       "1  5adf653f068401528a2aa697               656   \n",
       "2  5adf4626068401528a2aa628              2427   \n",
       "3  5adf40d2068401528a2aa619               626   \n",
       "4  5adf3d64068401528a2aa60f               815   \n",
       "\n",
       "                                      byline documentType  \\\n",
       "0                             By JOHN BRANCH      article   \n",
       "1                           By LISA FRIEDMAN      article   \n",
       "2                              By PETE WELLS      article   \n",
       "3  By JULIE HIRSCHFELD DAVIS and PETER BAKER      article   \n",
       "4             By IAN AUSTEN and DAN BILEFSKY      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Former N.F.L. Cheerleaders’ Settlement Offer: ...   \n",
       "1  E.P.A. to Unveil a New Rule. Its Effect: Less ...   \n",
       "2                            The New Noma, Explained   \n",
       "3                                            Unknown   \n",
       "4                                            Unknown   \n",
       "\n",
       "                                            keywords  multimedia     newDesk  \\\n",
       "0  ['Workplace Hazards and Violations', 'Football...          68      Sports   \n",
       "1  ['Environmental Protection Agency', 'Pruitt, S...          68     Climate   \n",
       "2  ['Restaurants', 'Noma (Copenhagen, Restaurant)...          66      Dining   \n",
       "3  ['Macron, Emmanuel (1977- )', 'Trump, Donald J...          68  Washington   \n",
       "4  ['Toronto, Ontario, Attack (April, 2018)', 'Mu...          68     Foreign   \n",
       "\n",
       "   printPage              pubDate   sectionName  \\\n",
       "0          0  2018-04-24 17:16:49  Pro Football   \n",
       "1          0  2018-04-24 17:11:21       Unknown   \n",
       "2          0  2018-04-24 14:58:44       Unknown   \n",
       "3          0  2018-04-24 14:35:57        Europe   \n",
       "4          0  2018-04-24 14:21:21        Canada   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  “I understand that they could meet with us, pa...  The New York Times   \n",
       "1  The agency plans to publish a new regulation T...  The New York Times   \n",
       "2  What’s it like to eat at the second incarnatio...  The New York Times   \n",
       "3  President Trump welcomed President Emmanuel Ma...  The New York Times   \n",
       "4  Alek Minassian, 25, a resident of Toronto’s Ri...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2018/04/24/sports/foot...  \n",
       "1           News  https://www.nytimes.com/2018/04/24/climate/epa...  \n",
       "2           News  https://www.nytimes.com/2018/04/24/dining/noma...  \n",
       "3           News  https://www.nytimes.com/2018/04/24/world/europ...  \n",
       "4           News  https://www.nytimes.com/2018/04/24/world/canad...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore the dataset using just one month of data\n",
    "\n",
    "April2018_headlines = pd.read_csv('~/take_home_tests/SynapseFI/data/articles/ArticlesApril2018.csv')\n",
    "April2018_headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8603"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load kaggle data\n",
    "\n",
    "curr_dir = '/Users/laurenfinkelstein/take_home_tests/SynapseFI/data/articles/'\n",
    "\n",
    "kaggle_headlines = []\n",
    "headline_counts = []\n",
    "\n",
    "for file in glob(curr_dir + '*.csv'):\n",
    "    article_df = pd.read_csv(file)\n",
    "    kaggle_headlines.extend(list(article_df.headline.values))\n",
    "    headline_counts.append(len(article_df.headline))\n",
    "\n",
    "kaggle_headlines = [h for h in kaggle_headlines if h != \"Unknown\"]\n",
    "len(kaggle_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of headlines in each month of Kaggle data: 1037.0\n"
     ]
    }
   ],
   "source": [
    "# calculate number of headlines in an average month in the Kaggle dataset\n",
    "\n",
    "mean_headlines = round(np.mean(headline_counts))\n",
    "print('Mean number of headlines in each month of Kaggle data:', mean_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Augment the data obtained from Kaggle with more data using the New York Times API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The New York Times headlines data downloaded from Kaggle includes data from the months of January - May 2017 and January - April 2018 (and hence is missing data from May 2018). To compensate for this missing month of data, I downloaded data directly using the New York Times Archive API from May 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NYT_headlines(year, month):\n",
    "    \n",
    "    '''\n",
    "     This function makes a request to the New York Times Archive API and collects \n",
    "     a list of article headlines for the specified month and year of interest.\n",
    "    '''\n",
    "\n",
    "    api_key = {'api-key' : pickle.load(open('apikey.pkl','rb'))}\n",
    "    url = 'https://api.nytimes.com/svc/archive/v1/' + str(year) + '/' + str(month) + '.json'\n",
    "    \n",
    "    response = requests.get(url, params=api_key)\n",
    "    output = response.json()\n",
    "    \n",
    "    docs = output['response']['docs']\n",
    "    \n",
    "    headlines = []\n",
    "    for doc in docs:\n",
    "        headlines.append(doc['headline']['main'])\n",
    "    \n",
    "    return headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of headlines in the NYT May 2018 Archive: 7421\n"
     ]
    }
   ],
   "source": [
    "may_2018_headlines = get_NYT_headlines(2018, 5)\n",
    "may_2018_headlines[:10]\n",
    "print ('Number of headlines in the NYT May 2018 Archive:', len(may_2018_headlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more headlines for May 2018 from the NYT Archive than for a given month from the Kaggle dataset. As to not bias the training data from Kaggle's dataset towards the NYT Archive May 2018 data, I will randomly sample a quantity of May 2018 headlines equal to the number of headlines in an average month of the Kaggle dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['With ‘Kudos,’ Rachel Cusk Completes an Exceptional Trilogy',\n",
       " '’80s Beauty Products That Are Still Beloved Today',\n",
       " 'How Tech Can Turn Doctors Into Clerical Workers',\n",
       " 'Every 202,500 Years, Earth Wanders in a New Direction',\n",
       " 'How the N.R.A. Fought Gun Control After Parkland']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.random.choice(range(1,len(may_2018_headlines)), int(mean_headlines))\n",
    "may_2018_rand = [may_2018_headlines[i] for i in index]\n",
    "may_2018_rand[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of May 2018 headlines retrieved from NYT archive: 1037\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of May 2018 headlines retrieved from NYT archive:\", len(may_2018_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge Kaggle data with NYT Archive data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to merge the data retrieved from the NYT Archive with the data downloaded from Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9640"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge Kaggle data with NYT Archive data\n",
    "\n",
    "all_headlines = kaggle_headlines + may_2018_rand\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've merged the headlines downloaded from Kaggle and the headlines downloaded using the NYT API together, we should check for duplicates and remove those from our list of headlines we will use to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate headlines: 449\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "\n",
    "len(all_headlines) == len(set(all_headlines))\n",
    "print(\"Number of duplicate headlines:\", len(all_headlines) - len(set(all_headlines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates in all_headlines\n",
    "\n",
    "seen = set()\n",
    "all_headlines_uniq = []\n",
    "for headline in all_headlines:\n",
    "    if headline not in seen:\n",
    "        all_headlines_uniq.append(headline)\n",
    "        seen.add(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# great, no more duplicate headlines\n",
    "\n",
    "len(all_headlines_uniq) == len(set(all_headlines_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of headlines in the training dataset: 9191\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of headlines in the training dataset:\", len(all_headlines_uniq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and tokenize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data consists of both cleaning and tokenizing the data. \n",
    "\n",
    "To clean the data, we will remove punctuation and lowercase all words in the corpus. We do not need to worry about removing stop words, as we do in many NLP projects, because we want to the model to generate fluid headlines similar to those that would be created by a human. \n",
    "\n",
    "To prepare the data for modeling by the neural network, we must convert words to integers (i.e., tokenize the data), as the words cannot be modeled directly. Tokenization is the process of extracting tokens (i.e., terms/words) from a corpus. \n",
    "\n",
    "We will clean and tokenize the corpus in the same step, using the Keras library's Tokenizer method. The tokenizer removes all punctuation including tabs and linebreaks, and converts text to lowercase. It then tokenizes/vectorizes all text in the corpus by turning each text into a sequence of integers/tokens. This step is important, as language modelling takes in sequential data (i.e., words/tokens), and uses this sequential data to predict the next word/token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sequence of n-gram tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've cleaned and tokenized the corpus, we will generate sequences of n-gram tokens that will be used to train the model. Each integer in these n-grams is the index of the given word in the complete vocabulary of words present in the corpus of text.\n",
    "\n",
    "For example, one of the headlines in the corpus is \"Finding an expansive view of a forgotten people in niger\". For this headline, the **sequences of n-gram tokens** will look like this: \n",
    "\n",
    "    [169, 17],\n",
    "    [169, 17, 665],\n",
    "    [169, 17, 665, 367],\n",
    "    [169, 17, 665, 367, 4],\n",
    "    [169, 17, 665, 367, 4, 2],\n",
    "    [169, 17, 665, 367, 4, 2, 666],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170, 5],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170, 5, 667]\n",
    "\n",
    "which, respectively, equate to the following **n-grams of text**: \n",
    "\n",
    "    Finding an,\n",
    "    Finding an expansive,\n",
    "    Finding an expansive view,\n",
    "    Finding an expansive view of,\n",
    "    Finding an expansive view of a,\n",
    "    Finding an expansive view of a forgotten,\n",
    "    Finding an expansive view of a forgotten people,\n",
    "    Finding an expansive view of a forgotten people in,\n",
    "    Finding an expansive view of a forgotten people in niger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the first 5 input sequences...\n",
      "[[394, 18], [394, 18, 5626], [394, 18, 5626, 553], [394, 18, 5626, 553, 4], [394, 18, 5626, 553, 4, 2], [394, 18, 5626, 553, 4, 2, 1502], [394, 18, 5626, 553, 4, 2, 1502, 159], [394, 18, 5626, 553, 4, 2, 1502, 159, 5], [394, 18, 5626, 553, 4, 2, 1502, 159, 5, 2180], [7, 74]]\n",
      "\n",
      "Total number of samples in input_sequences: 322255\n",
      "\n",
      "Total number of words in the vocabulary: 12861\n"
     ]
    }
   ],
   "source": [
    "corpus = all_headlines_uniq\n",
    "\n",
    "# Clean and tokenize using Keras' built-in Tokenizer() method\n",
    "tokenizer = Tokenizer() # create the tokenizer\n",
    "tokenizer.fit_on_texts(corpus) # fit the tokenizer on the documents\n",
    "\n",
    "# convert data to sequence of tokens\n",
    "input_sequences = [] # sequences of n-grams from all documents in the corpus\n",
    "for headline in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([headline])[0] # list of tokens corresponding to each word in the document (i.e., news headline)\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1] # creates n-gram sequence (from 2 word n-grams, through n-grams = len(headline))\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "print(\"Printing the first 5 input sequences...\")\n",
    "print(input_sequences[:10])\n",
    "\n",
    "nb_samples = sum(len(s) for s in input_sequences) # total number of samples in input_sequences; \n",
    "print(\"\\nTotal number of samples in input_sequences:\", nb_samples)\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"\\nTotal number of words in the vocabulary:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we've generated a data-set containing a sequence of tokens. It's important to recognize that different sequences may have different lengths. Before training the model, we must ensure all sequence lengths are equal. For this, we can pad the sequences using the Keras pad_sequence function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(s) for s in input_sequences]) # find length of the longest input sequence (i.e., headline)\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')) # pads the input sequences and converts to a numpy array (must fit model on np arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Obtain variables (predictor and target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define the training data for the network. In order to input data and train the model, we must create predictors and a label. We will use a n-gram sequence as predictors, and the next word of the n-gram as the label. \n",
    "\n",
    "For example, for the headline \"Finding an expansive view of a forgotten people in niger\", the first three **predictors** will be:\n",
    "\n",
    "    1. Finding an\n",
    "    2. Finding an expansive\n",
    "    3. Finding an expansive view\n",
    "\n",
    "and the corresponding **labels** will be\n",
    "\n",
    "    1. expansive\n",
    "    2. view\n",
    "    3. of\n",
    "\n",
    "... and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1] # label is the last word of each n-gram, and predictors are all the preceeding words\n",
    "label = ku.to_categorical(label, num_classes=vocab_size) # one-hot encodes the labels; matrix dimension are the number of input sequences by the number of total words/tokens in the word dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can obtain the input vector *X* and the label vector _Y_, which will be used to train the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Build and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our LSTM network. Let's start with a simple LSTM, and then later add more layers and increase the number of memory units and epochs. Then we can compare the accuracy, loss, and results to ultimately choose the best network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a network consisting of a three layer archiecture, including:\n",
    "       \n",
    "***Input (Embedding) Layer*** : Takes the sequence of words as input, represented as dense vectors\n",
    "<br /> ***LSTM Layer*** : Hidden layer; computes the output using LSTM units (100 units in this case)\n",
    "<br /> ***Output (Dense) Layer*** : Dense layer; computes the probability of the best possible next word as the output\n",
    "    \n",
    "We will start by running the model for 100 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = max_sequence_len - 1 # length of the longest sequence - the length of the predictor (length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Keras Sequential model \n",
    "model = Sequential()\n",
    "\n",
    "## Specify the network architecture\n",
    "model.add(Embedding(vocab_size, 50, input_length=input_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 27, 50)            643050    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12861)             1298961   \n",
      "=================================================================\n",
      "Total params: 2,002,411\n",
      "Trainable params: 2,002,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the network (sets up training parameters before training)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the model architecture, we can train it using our data (predictors and labels).\n",
    "\n",
    "Because the network is slow to train, we can use model checkpointing to record all the network weights anytime an improvement in loss occurs at the end of an epoch. We can then use the best set of weights (i.e., the lowest loss) to later instantiate the text generative model for use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "model1_hist = model.fit(predictors, label, epochs=100, verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb')) # pickle the tokenizer for use in external script\n",
    "model.save('model_1.hdf5') # save the model for later use / use in external script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up training, the network was trained on a AWS GPU instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model1 = load_model('model_1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_loss(model_history):\n",
    "    \n",
    "    '''\n",
    "    This function plots the learning and loss curves for a neural network. \n",
    "    Takes model history as input. \n",
    "    '''\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    \n",
    "    # plot learning curve\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(model_history.history['acc'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig('./accuracy_curve.png')\n",
    "  \n",
    "    # summarize history for loss\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(model_history.history['loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig('./loss_curve.png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_loss(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Use the model to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been trained, we can use the network to generate text. We use a seed sequence as the input and predict the next word based on input words. \n",
    "\n",
    "To feed this seed text into the model, we will need to do some pre-processing, which includes (1) tokenizing the seed text and (2) padding the sequences, similar to the way we preprocessed the text sequences the model was trained on. We can then pass the sequences into the trained model to get the predicted word. The predicted words can then be appended together to obtain a predicted sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_headline(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] # list of tokens corresponding to each word in the document (i.e., news headline)\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # add padding\n",
    "        predicted = model.predict_classes(token_list, verbose=1) # make predictions using trained model\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items(): # word index gives a dictionary of words and their uniquely assigned integers\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "Donald Trump Myth Of Women’S Stocks\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "New York For End And Subway\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Uk Brexit Allow Class The Pay Our British\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "Vladimir Putin On Dies Of Needs\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "Opioid Crisis He Injury For Disaster\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "Barack Obama Of Trump’S Culture Disability\n"
     ]
    }
   ],
   "source": [
    "print (generate_headline(\"Donald Trump\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"New York\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"UK Brexit\", 6, model1, max_sequence_len))\n",
    "print (generate_headline(\"Vladimir Putin\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"Opioid Crisis\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"Barack Obama\",4, model1, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In observing the generated text from Model 1, we see that some of the sequences make sense, but not all are perfect. \n",
    "\n",
    "Some ideas to improve the quality of the results, by fine-tuning the network architecture and parameters, include:\n",
    "- using a larger network\n",
    "- increasing the input dimension in the embedding layer\n",
    "- increasing the number of memory units in the LSTM layer(s)\n",
    "- increasing the number of training epochs and decreasing the batch size\n",
    "\n",
    "These improvement ideas will be experimented on in Models 2 and 3, below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try creating a larger network by adding a second LSTM layer to see if this improves the quality of the generated text. Let's start by keeping the number of memory units the same at 100, but adding a second LSTM layer. So the new model architecture becomes:\n",
    "\n",
    "***Input (Embedding) Layer***\n",
    "<br /> ***LSTM Layer***\n",
    "<br /> ***LSTM Layer***\n",
    "<br /> ***Output (Dense) Layer***\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, 50, input_length=input_len)) # try increasing output dim to 100\n",
    "model2.add(LSTM(100, return_sequences=True))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_hist = model2.fit(predictors, label, epochs=100, verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('model_2.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_headline(\"Donald Trump\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"New York\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"UK Brexit\", 6, model1, max_sequence_len))\n",
    "print (generate_headline(\"Vladimir Putin\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"Opioid Crisis\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"Barack Obama\",4, model1, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same architecture as model 2, but try increasing the output dimension in the embedding layer to 100 and increasing number of memory units in the LSTM layers to 200. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(vocab_size, 100, input_length=input_len))\n",
    "model3.add(LSTM(200, return_sequences=True))\n",
    "model3.add(LSTM(200))\n",
    "model3.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try increasing the number of training epochs from 100 to 150 and decreasing the batch size from unspecified (i.e., 32) to 16. This affords the network a greater opportunity to learn and update its weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_hist = model3.fit(predictors, label, epochs=150, batch_size=16, verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('model_3.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_headline(\"Donald Trump\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"New York\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"UK Brexit\", 6, model1, max_sequence_len))\n",
    "print (generate_headline(\"Vladimir Putin\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"Opioid Crisis\", 4, model1, max_sequence_len))\n",
    "print (generate_headline(\"Barack Obama\",4, model1, max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
