{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News headline text generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I create a model that generates news headlines using the Kaggle dataset of New York Times comments and headlines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISC data science libraries\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "import requests # for accesing the NYT API\n",
    "import pickle\n",
    "from pickle import load\n",
    "\n",
    "\n",
    "# keras module for preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.utils as ku \n",
    "# keras module for building LSTM \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# keras module for LSTM training\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# set seeds for reproducability\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(2)\n",
    "seed(1)\n",
    "\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load data\n",
    "\n",
    "### 1. Load the [kaggle dataset](https://www.kaggle.com/aashita/nyt-comments) of New York Times comments and headlines\n",
    "The NYT dataset downloaded from Kaggle contains eight months of data, from the months January, February, March, and April of 2017 and 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781</td>\n",
       "      <td>By JOHN BRANCH</td>\n",
       "      <td>article</td>\n",
       "      <td>Former N.F.L. Cheerleaders’ Settlement Offer: ...</td>\n",
       "      <td>['Workplace Hazards and Violations', 'Football...</td>\n",
       "      <td>68</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:16:49</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>“I understand that they could meet with us, pa...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/sports/foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5adf653f068401528a2aa697</td>\n",
       "      <td>656</td>\n",
       "      <td>By LISA FRIEDMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>E.P.A. to Unveil a New Rule. Its Effect: Less ...</td>\n",
       "      <td>['Environmental Protection Agency', 'Pruitt, S...</td>\n",
       "      <td>68</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:11:21</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>The agency plans to publish a new regulation T...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/climate/epa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5adf4626068401528a2aa628</td>\n",
       "      <td>2427</td>\n",
       "      <td>By PETE WELLS</td>\n",
       "      <td>article</td>\n",
       "      <td>The New Noma, Explained</td>\n",
       "      <td>['Restaurants', 'Noma (Copenhagen, Restaurant)...</td>\n",
       "      <td>66</td>\n",
       "      <td>Dining</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:58:44</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>What’s it like to eat at the second incarnatio...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/dining/noma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5adf40d2068401528a2aa619</td>\n",
       "      <td>626</td>\n",
       "      <td>By JULIE HIRSCHFELD DAVIS and PETER BAKER</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Macron, Emmanuel (1977- )', 'Trump, Donald J...</td>\n",
       "      <td>68</td>\n",
       "      <td>Washington</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:35:57</td>\n",
       "      <td>Europe</td>\n",
       "      <td>President Trump welcomed President Emmanuel Ma...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5adf3d64068401528a2aa60f</td>\n",
       "      <td>815</td>\n",
       "      <td>By IAN AUSTEN and DAN BILEFSKY</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Toronto, Ontario, Attack (April, 2018)', 'Mu...</td>\n",
       "      <td>68</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:21:21</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Alek Minassian, 25, a resident of Toronto’s Ri...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/canad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID  articleWordCount  \\\n",
       "0  5adf6684068401528a2aa69b               781   \n",
       "1  5adf653f068401528a2aa697               656   \n",
       "2  5adf4626068401528a2aa628              2427   \n",
       "3  5adf40d2068401528a2aa619               626   \n",
       "4  5adf3d64068401528a2aa60f               815   \n",
       "\n",
       "                                      byline documentType  \\\n",
       "0                             By JOHN BRANCH      article   \n",
       "1                           By LISA FRIEDMAN      article   \n",
       "2                              By PETE WELLS      article   \n",
       "3  By JULIE HIRSCHFELD DAVIS and PETER BAKER      article   \n",
       "4             By IAN AUSTEN and DAN BILEFSKY      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Former N.F.L. Cheerleaders’ Settlement Offer: ...   \n",
       "1  E.P.A. to Unveil a New Rule. Its Effect: Less ...   \n",
       "2                            The New Noma, Explained   \n",
       "3                                            Unknown   \n",
       "4                                            Unknown   \n",
       "\n",
       "                                            keywords  multimedia     newDesk  \\\n",
       "0  ['Workplace Hazards and Violations', 'Football...          68      Sports   \n",
       "1  ['Environmental Protection Agency', 'Pruitt, S...          68     Climate   \n",
       "2  ['Restaurants', 'Noma (Copenhagen, Restaurant)...          66      Dining   \n",
       "3  ['Macron, Emmanuel (1977- )', 'Trump, Donald J...          68  Washington   \n",
       "4  ['Toronto, Ontario, Attack (April, 2018)', 'Mu...          68     Foreign   \n",
       "\n",
       "   printPage              pubDate   sectionName  \\\n",
       "0          0  2018-04-24 17:16:49  Pro Football   \n",
       "1          0  2018-04-24 17:11:21       Unknown   \n",
       "2          0  2018-04-24 14:58:44       Unknown   \n",
       "3          0  2018-04-24 14:35:57        Europe   \n",
       "4          0  2018-04-24 14:21:21        Canada   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  “I understand that they could meet with us, pa...  The New York Times   \n",
       "1  The agency plans to publish a new regulation T...  The New York Times   \n",
       "2  What’s it like to eat at the second incarnatio...  The New York Times   \n",
       "3  President Trump welcomed President Emmanuel Ma...  The New York Times   \n",
       "4  Alek Minassian, 25, a resident of Toronto’s Ri...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2018/04/24/sports/foot...  \n",
       "1           News  https://www.nytimes.com/2018/04/24/climate/epa...  \n",
       "2           News  https://www.nytimes.com/2018/04/24/dining/noma...  \n",
       "3           News  https://www.nytimes.com/2018/04/24/world/europ...  \n",
       "4           News  https://www.nytimes.com/2018/04/24/world/canad...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore the dataset using just one month of data\n",
    "\n",
    "April2018_headlines = pd.read_csv('/Users/laurenfinkelstein/take_home_tests/SynapseFI/data/ArticlesApril2018.csv')\n",
    "April2018_headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method from Kaggle kernel\n",
    "# load all eight months of data available through Kaggle\n",
    "\n",
    "curr_dir = '/Users/laurenfinkelstein/take_home_tests/SynapseFI/data/'\n",
    "kaggle_headlines = []\n",
    "for filename in os.listdir(curr_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(curr_dir + filename)\n",
    "        kaggle_headlines.extend(list(article_df.headline.values))\n",
    "        break\n",
    "\n",
    "kaggle_headlines = [h for h in kaggle_headlines if h != \"Unknown\"]\n",
    "len(kaggle_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# William method? using glob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Augment the data obtained from Kaggle with more data using the New York Times API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After first running the model using just the data obtained from Kaggle, I saw that the results were unsatisfactory. This may be due to the fact that the Kaggle dataset only includes eight months worth of data (Jan, Feb, March, and April of 2017 and 2018), totaling only 831 headlines. To improve the model, I've augmented the dataset with headlines from the months of January 2019 through April 2019 using data obtained directly from the New York Times API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NYT_headlines(year, month):\n",
    "    \n",
    "    '''\n",
    "     This function makes a request to the New York Times Archive API and collects \n",
    "     a list of article headlines for the specified month and year of interest.\n",
    "    '''\n",
    "\n",
    "    api_key = {'api-key' : '6dZ0JG8qQwUG4Wfs6JuukwOhw2SJxZ1f'}\n",
    "    url = 'https://api.nytimes.com/svc/archive/v1/' + str(year) + '/' + str(month) + '.json'\n",
    "    \n",
    "    response = requests.get(url, params=api_key)\n",
    "    output = response.json()\n",
    "    \n",
    "    docs = output['response']['docs']\n",
    "    \n",
    "    headlines = []\n",
    "    for doc in docs:\n",
    "        headlines.append(doc['headline']['main'])\n",
    "    \n",
    "    return headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The People ‘Are Hungry’: Scenes from the ‘Yellow Vest’ Protests in Paris',\n",
       " 'The Marvelous Mr. Mackie',\n",
       " 'Brazil’s New Leader Wants to Ease Gun Laws. Supporters Are Ready, and Training.',\n",
       " 'George H.W. Bush, Public Servant',\n",
       " 'Politicians and Family React to George Bush’s Death',\n",
       " 'How George Bush Befriended Dana Carvey, the ‘S.N.L.’ Comedian Who Impersonated Him',\n",
       " 'Quotation of the Day: A Peaceful Exit, but First, One Last ‘I Love You, Too’',\n",
       " 'The Most Wonderful Smelling Time of the Year',\n",
       " 'Michelle Bachelet: Ignore Climate Change at Your Peril',\n",
       " 'N.Y. Today: Protecting Rent-Stabilized Tenants From Shady Landlords']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the above function using the month December, 2018\n",
    "\n",
    "dec_2018_headlines = get_NYT_headlines(2018, 12)\n",
    "dec_2018_headlines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_range_len(start_yr, start_mth, end_yr, end_mth):\n",
    "    months = (end_yr - start_yr)*12 + (end_mth - start_mth) + 1\n",
    "    return months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_df(months, start_yr, start_mth):\n",
    "    yr_list = []\n",
    "    mth_list = []\n",
    "    date_list = []\n",
    "    curr_yr = start_yr\n",
    "    curr_mth = start_mth\n",
    "    for i in range(months):\n",
    "        yr_list.append(curr_yr)\n",
    "        mth_list.append(curr_mth)\n",
    "        date_list.append(dt.date(curr_yr, curr_mth, 1))\n",
    "        if curr_mth < 12:\n",
    "            curr_mth += 1\n",
    "        else:\n",
    "            curr_mth = 1\n",
    "            curr_yr += 1\n",
    "    date_df = pd.DataFrame(yr_list, columns=['year'])\n",
    "    date_df['month'] = mth_list\n",
    "    date_df['date'] = date_list\n",
    "    return date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download data from January 2019 through April 2019\n",
    "\n",
    "start_yr = 2019\n",
    "start_mth = 1\n",
    "end_yr = 2019\n",
    "end_mth = 4\n",
    "\n",
    "months = date_range_len(start_yr, start_mth, end_yr, end_mth)\n",
    "months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month        date\n",
       "0  2019      1  2019-01-01\n",
       "1  2019      2  2019-02-01\n",
       "2  2019      3  2019-03-01\n",
       "3  2019      4  2019-04-01"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_df = get_date_df(months, start_yr, start_mth)\n",
    "date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 1\n",
      "2019 2\n",
      "2019 3\n",
      "2019 4\n"
     ]
    }
   ],
   "source": [
    "NYT_headlines = []\n",
    "for i, yr in enumerate(date_df['year']):\n",
    "    print(yr, date_df['month'][i])\n",
    "    headlines = get_NYT_headlines(yr, date_df['month'][i])\n",
    "#     headlines_list_2019.append(headlines)\n",
    "    NYT_headlines.extend(list(headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27822"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NYT_headlines) \n",
    "\n",
    "# that's better; now we have a lot more data to train our model with \n",
    "# and can append this to the data downloaded from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Daryl Dragon, of the Captain and Tennille Pop Duo, Dies at 76',\n",
       " 'Where Doulas Calm Nerves and Bridge Cultures During Childbirth',\n",
       " 'Voting Issues and Gerrymanders Are Now Key Political Battlegrounds',\n",
       " 'Protecting Pregnant Workers',\n",
       " 'When Louis C.K. Crossed a Line']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYT_headlines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are a lot more headlines from the four months of data downloaded using the NYT API\n",
    "# than there are in the dataset downloaded from Kaggle.\n",
    "# Are there duplicates?\n",
    "\n",
    "len(NYT_headlines) == len(set(NYT_headlines))\n",
    "# len(headlines_2019) - len(set(headlines_2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like there are many (12,750) duplicate headlines in headlines_2019; let's get rid of those\n",
    "\n",
    "seen = set()\n",
    "NYT_headlines_uniq = []\n",
    "for headline in NYT_headlines:\n",
    "    if headline not in seen:\n",
    "        NYT_headlines_uniq.append(headline)\n",
    "        seen.add(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NYT_headlines_uniq) == len(set(NYT_headlines_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks like there are also some duplicates in the Kaggle dataset, but only 2\n",
    "\n",
    "len(kaggle_headlines) == len(set(kaggle_headlines))\n",
    "len(kaggle_headlines) - len(set(kaggle_headlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge Kaggle data with NYT Archive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15903"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge Kaggle data with NYT Archive data\n",
    "\n",
    "all_headlines = kaggle_headlines + NYT_headlines_uniq\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've merged the headlines downloaded from Kaggle and the headlines downloaded using the NYT API together, we need to check for duplicates and remove those from our list of headlines we will use to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 3 duplicates in our all_headlines list\n",
    "\n",
    "len(all_headlines) == len(set(all_headlines))\n",
    "len(all_headlines) - len(set(all_headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get rid of the duplicates in all_headlines\n",
    "\n",
    "seen2 = set()\n",
    "all_headlines_uniq = []\n",
    "for headline in all_headlines:\n",
    "    if headline not in seen2:\n",
    "        all_headlines_uniq.append(headline)\n",
    "        seen2.add(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15900"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# great, no more duplicate headlines\n",
    "\n",
    "len(all_headlines_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like some of the headlines in this list are simply empty strings. We will remove these from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looks like there is an empty string in all_headlines_uniq\n",
    "\n",
    "'' in all_headlines_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get rid of that empty string\n",
    "\n",
    "all_headlines_uniq.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'' in all_headlines_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looks like there are null values too\n",
    "\n",
    "None in all_headlines_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null values\n",
    "\n",
    "all_headlines_uniq.remove(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "None in all_headlines_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15898"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_headlines_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean and tokenize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data consists of both cleaning and tokenizing the data. To clean the data, we will remove punctuation and lowercase all words in the corpus. We do not need to worry about removing stop words, as we do in many NLP projects, because we want to the model to generate fluid headlines similar to those that would be created by a human. \n",
    "\n",
    "We will clean and tokenize the corpus in the same step, using the Keras library's Tokenizer method. The tokenizer removes all punctuation as well as tabs and linebreaks, converts text to lowercase, and tokenizes/vectorizes all text in the corpus by turning each text into a sequence of integers. \n",
    "\n",
    "Tokenization is necessary for preparing data for embedding layer (see model architecture section below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[590, 21, 4191, 675, 5, 2, 2704, 163, 4, 3059], [6, 61, 1, 4192, 13, 4193], [613, 9767, 73, 4194], [6588, 9768, 769, 1569, 925], [614, 377, 7, 2435]]\n",
      "869840\n",
      "20387\n"
     ]
    }
   ],
   "source": [
    "corpus = all_headlines_uniq\n",
    "\n",
    "# max_words = 50000 # Max size of the dictionary (from Jeremy)\n",
    "\n",
    "# For simplicity, one \"sentence\" per line \n",
    "# corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2] # from Metis Deep Learning\n",
    "\n",
    "# Clean and tokenize using Keras' built-in Tokenizer() method\n",
    "tokenizer = Tokenizer() # create the tokenizer\n",
    "tokenizer.fit_on_texts(corpus) # fit the tokenizer on the documents\n",
    "\n",
    "# Convert tokenized texts to sequence format\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "print(sequences[:5])\n",
    "nb_samples = sum(len(s) for s in corpus) # number of samples total in sequences; \n",
    "print(nb_samples)\n",
    "\n",
    "# Vocab size ## FROM METIS DEEP LEARNING\n",
    "V = len(tokenizer.word_index) + 1 # total words (WHY DO WE ADD 1??); word index gives a dictionary of words and their uniquely assigned integers\n",
    "print(V)\n",
    "\n",
    "# Dimension to reduce to\n",
    "dim = 100\n",
    "window_size = 2\n",
    "# sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[590, 21],\n",
       "  [590, 21, 4191],\n",
       "  [590, 21, 4191, 675],\n",
       "  [590, 21, 4191, 675, 5],\n",
       "  [590, 21, 4191, 675, 5, 2]],\n",
       " 20387)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert data to sequence of tokens\n",
    "\n",
    "# sequence of n-grams from all documents in the corpus\n",
    "input_sequences = [] \n",
    "\n",
    "for headline in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([headline])[0] # list of tokens corresponding to each word in the document (i.e., news headline)\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1] # creates n-gram sequence (from 2 word n-grams, through n-grams = len(line))\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "input_sequences[:5], V\n",
    "        \n",
    "# print(input_sequences1, V)\n",
    "\n",
    "# inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "# inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## FROM JEREMY CODE, BUT SHOuLD I DO THIS? PROB NOT BECAUSE EACH HEADLINE SHOULD BE SEPARATE...\n",
    "\n",
    "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
    "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## FROM KAGGLE CODE\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus) # fit the tokenizer on the documents\n",
    "    total_words = len(tokenizer.word_index) + 1 # word index gives a dictionary of words and their uniquely assigned integers\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = [] # sequence of n-grams from all documents in the corpus\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0] # list of tokens corresponding to each word in the document (i.e., news headline)\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence) # sequence of n-grams (from 2 word n-grams, through n-grams = len(line))\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "\n",
    "- remove punctuation\n",
    "- lowercase all words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_headlines_uniq[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    '''\n",
    "    Takes in some text as input, and removes punctuation and lowercases all words.\n",
    "    '''\n",
    "    \n",
    "    text = ''\n",
    "    for word in text:\n",
    "        if word not in string.punctuation:\n",
    "            print(word)\n",
    "            word = word.lower()\n",
    "            print(word)\n",
    "            text.join(word)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_corpus = []\n",
    "\n",
    "for headline in all_headlines_uniq[:10]:\n",
    "    h = clean_text(headline)\n",
    "    corpus.append(h)\n",
    "\n",
    "test_corpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower() # removes punctuation and lowercases words\n",
    "#     txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore') # converts from utf8 to ascii -- REMOVE THIS!!\n",
    "    return txt \n",
    "\n",
    "test_corpus2 = [clean_text(h) for h in all_headlines_uniq]\n",
    "test_corpus2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate sequence of n-gram tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modelling takes in sequential data (i.e., words/tokens), and uses this sequential data to predict the next word/token. This requires tokenization, which is the process of extracting tokens (i.e., terms/words) from a corpus. To do this, I use the Python library Keras' built-in model for tokenization, which is able to obtain the tokens and their respsective index in the corpus. This allows for every text document in the dataset to be converted into a sequence of tokens."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus) # fit the tokenizer on the documents\n",
    "    total_words = len(tokenizer.word_index) + 1 # word index gives a dictionary of words and their uniquely assigned integers\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = [] # sequence of n-grams from all documents in the corpus\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0] # list of tokens corresponding to each word in the document (i.e., news headline)\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence) # sequence of n-grams (from 2 word n-grams, through n-grams = len(line))\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each list in the above output represents the ngram phrases generated from the input data (i.e., the documents/headlines). Each integer in these n-grams is the index of the given word in the complete vocabulary of words present in the corpus of text.\n",
    "\n",
    "For example, the first headline in the corpus is \"Finding an expansive view of a forgotten people in niger\". For this headline, we see the following output, i.e. **sequences of tokens**:\n",
    "\n",
    "    [169, 17],\n",
    "    [169, 17, 665],\n",
    "    [169, 17, 665, 367],\n",
    "    [169, 17, 665, 367, 4],\n",
    "    [169, 17, 665, 367, 4, 2],\n",
    "    [169, 17, 665, 367, 4, 2, 666],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170, 5],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170, 5, 667]\n",
    "\n",
    "which, respectively, equate to the following **n-grams**: \n",
    "\n",
    "    Finding an,\n",
    "    Finding an expansive,\n",
    "    Finding an expansive view,\n",
    "    Finding an expansive view of,\n",
    "    Finding an expansive view of a,\n",
    "    Finding an expansive view of a forgotten,\n",
    "    Finding an expansive view of a forgotten people,\n",
    "    Finding an expansive view of a forgotten people in,\n",
    "    Finding an expansive view of a forgotten people in niger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pad the sequences and obtain variables (predictor and target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we've generated a data-set containing a sequence of tokens. It's important to recognize that different sequences may have different lengths. Before training the model, we must ensure all sequence lengths are equal. For this, we can pad the sequences using the Keras pad_sequence function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pad the sequences (CODE COMES FROM KAGGLE GUY)\n",
    "\n",
    "# find length of the longest input sequence (i.e., headline)\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')) # pads the input sequences and converts to a numpy array (must fit model on np arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, inorder to input data and train the model, we must create predictors and a label. We will use a n-grams sequence as predictors, and the next word of the n-gram as the label. \n",
    "\n",
    "For example, for the headline \"Finding an expansive view of a forgotten people in niger\", the **predictors** will be:\n",
    "\n",
    "    1. Finding an\n",
    "    2. Finding an expansive\n",
    "    3. Finding an expansive view\n",
    "\n",
    "and the corresponding **labels** will be\n",
    "\n",
    "    1. expansive\n",
    "    2. view\n",
    "    3. of\n",
    "\n",
    "... and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1] # label is the last word of each n-gram, and predictors are all the preceeding words\n",
    "label = ku.to_categorical(label, num_classes=V) # one-hot encodes the labels; this is a matrix of 4,806 rows (the number of input sequences) and 2,422 columns (the number of total words/tokens in the word dictionary)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## FUNCTION FROM KAGGLE GUY\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    ## pad the sequences\n",
    "    max_sequence_len = max([len(x) for x in input_sequences]) # find length of the longest input sequence (i.e., headline)\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')) # pads the input sequences and converts to a numpy array (must fit model on np arrays)\n",
    "    \n",
    "    ## create predictors and label\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1] # label is the last word of each n-gram, and predictors are all the preceeding words\n",
    "    label = ku.to_categorical(label, num_classes=total_words) # one-hot encodes the labels; this is a matrix of 4,806 rows (the number of documents/headlines in the corpus) and 2,422 columns (the number of words/tokens in the corpus (i.e., length of input_sequences))\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can obtain the input vector *X* and the label vector _Y_, which will be used for to training the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Build and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a LSTM model with a three layer architecture, including:\n",
    "    \n",
    "***Input Layer*** : Takes the sequence of words as input\n",
    "<br /> ***LSTM Layer*** : Computes the output using LSTM units. There are currently 100 units, but this can be fine-tuned later.\n",
    "<br /> ***Dropout Layer*** : A regularization layer which randomly turns off the activations of some neurons in the LSTM layer in order to prevent overfitting. (Note: this is an optional layer)\n",
    "<br /> ***Output Layer*** : Computes the probability of the best possible next word as output\n",
    "    \n",
    "We will run this model for  100 epochs, but this can be further experimented with and fine-tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the model\n",
    "\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1 # - 1 because the last word in the sequence is used as the label (?)\n",
    "    \n",
    "    ## Create a Keras Sequential model \n",
    "    model = Sequential()\n",
    "    \n",
    "    ## Specify the network architecture\n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    # Compile the network (sets up training parameters before training)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the network graphically\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the model architecture, we can train it using our data (predictors and labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the model\n",
    "\n",
    "model.fit(predictors, label, epochs=100, verbose=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Use the model to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been trained, we can use it to predict the next word based on input words (i.e., seed text). \n",
    "\n",
    "To feed this seed text into the model, we will need to do some pre-processing, which includes (1) tokenizing the seed text and (2) padding the sequences. We can then pass the sequences into the trained model to get the predicted word. The multiple predicted words can then be appended together to obtain a predicted sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] # list of tokens corresponding to each word in the document (i.e., news headline)\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # add padding\n",
    "        predicted = model.predict_classes(token_list, verbose=0) # make predictions using trained model\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items(): # word index gives a dictionary of words and their uniquely assigned integers\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"united states\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"president trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"donald trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"new york\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"science and technology\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (?)\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('\\n', loss_and_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement Ideas\n",
    "As we can see, the model has produced the output which looks fairly fine. The results can be improved further with following points:\n",
    "\n",
    "- Adding more data\n",
    "- Fine Tuning the network architecture\n",
    "- Fine Tuning the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
