{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News headline text generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I create a model that generates news headlines using the Kaggle dataset of New York Times comments and headlines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# MISC data science libraries\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "import requests # for accesing the NYT API\n",
    "import pickle\n",
    "from pickle import load\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "# keras module for preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.utils as ku \n",
    "# keras module for building LSTM \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# keras module for LSTM training\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# set seeds for reproducability\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(2)\n",
    "seed(1)\n",
    "\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load data\n",
    "\n",
    "### 1. Load the [kaggle dataset](https://www.kaggle.com/aashita/nyt-comments) of New York Times comments and headlines\n",
    "The NYT dataset downloaded from Kaggle contains eight months of data, from the months January, February, March, and April of 2017 and 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781</td>\n",
       "      <td>By JOHN BRANCH</td>\n",
       "      <td>article</td>\n",
       "      <td>Former N.F.L. Cheerleaders’ Settlement Offer: ...</td>\n",
       "      <td>['Workplace Hazards and Violations', 'Football...</td>\n",
       "      <td>68</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:16:49</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>“I understand that they could meet with us, pa...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/sports/foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5adf653f068401528a2aa697</td>\n",
       "      <td>656</td>\n",
       "      <td>By LISA FRIEDMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>E.P.A. to Unveil a New Rule. Its Effect: Less ...</td>\n",
       "      <td>['Environmental Protection Agency', 'Pruitt, S...</td>\n",
       "      <td>68</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:11:21</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>The agency plans to publish a new regulation T...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/climate/epa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5adf4626068401528a2aa628</td>\n",
       "      <td>2427</td>\n",
       "      <td>By PETE WELLS</td>\n",
       "      <td>article</td>\n",
       "      <td>The New Noma, Explained</td>\n",
       "      <td>['Restaurants', 'Noma (Copenhagen, Restaurant)...</td>\n",
       "      <td>66</td>\n",
       "      <td>Dining</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:58:44</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>What’s it like to eat at the second incarnatio...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/dining/noma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5adf40d2068401528a2aa619</td>\n",
       "      <td>626</td>\n",
       "      <td>By JULIE HIRSCHFELD DAVIS and PETER BAKER</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Macron, Emmanuel (1977- )', 'Trump, Donald J...</td>\n",
       "      <td>68</td>\n",
       "      <td>Washington</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:35:57</td>\n",
       "      <td>Europe</td>\n",
       "      <td>President Trump welcomed President Emmanuel Ma...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5adf3d64068401528a2aa60f</td>\n",
       "      <td>815</td>\n",
       "      <td>By IAN AUSTEN and DAN BILEFSKY</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Toronto, Ontario, Attack (April, 2018)', 'Mu...</td>\n",
       "      <td>68</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:21:21</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Alek Minassian, 25, a resident of Toronto’s Ri...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/canad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID  articleWordCount  \\\n",
       "0  5adf6684068401528a2aa69b               781   \n",
       "1  5adf653f068401528a2aa697               656   \n",
       "2  5adf4626068401528a2aa628              2427   \n",
       "3  5adf40d2068401528a2aa619               626   \n",
       "4  5adf3d64068401528a2aa60f               815   \n",
       "\n",
       "                                      byline documentType  \\\n",
       "0                             By JOHN BRANCH      article   \n",
       "1                           By LISA FRIEDMAN      article   \n",
       "2                              By PETE WELLS      article   \n",
       "3  By JULIE HIRSCHFELD DAVIS and PETER BAKER      article   \n",
       "4             By IAN AUSTEN and DAN BILEFSKY      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Former N.F.L. Cheerleaders’ Settlement Offer: ...   \n",
       "1  E.P.A. to Unveil a New Rule. Its Effect: Less ...   \n",
       "2                            The New Noma, Explained   \n",
       "3                                            Unknown   \n",
       "4                                            Unknown   \n",
       "\n",
       "                                            keywords  multimedia     newDesk  \\\n",
       "0  ['Workplace Hazards and Violations', 'Football...          68      Sports   \n",
       "1  ['Environmental Protection Agency', 'Pruitt, S...          68     Climate   \n",
       "2  ['Restaurants', 'Noma (Copenhagen, Restaurant)...          66      Dining   \n",
       "3  ['Macron, Emmanuel (1977- )', 'Trump, Donald J...          68  Washington   \n",
       "4  ['Toronto, Ontario, Attack (April, 2018)', 'Mu...          68     Foreign   \n",
       "\n",
       "   printPage              pubDate   sectionName  \\\n",
       "0          0  2018-04-24 17:16:49  Pro Football   \n",
       "1          0  2018-04-24 17:11:21       Unknown   \n",
       "2          0  2018-04-24 14:58:44       Unknown   \n",
       "3          0  2018-04-24 14:35:57        Europe   \n",
       "4          0  2018-04-24 14:21:21        Canada   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  “I understand that they could meet with us, pa...  The New York Times   \n",
       "1  The agency plans to publish a new regulation T...  The New York Times   \n",
       "2  What’s it like to eat at the second incarnatio...  The New York Times   \n",
       "3  President Trump welcomed President Emmanuel Ma...  The New York Times   \n",
       "4  Alek Minassian, 25, a resident of Toronto’s Ri...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2018/04/24/sports/foot...  \n",
       "1           News  https://www.nytimes.com/2018/04/24/climate/epa...  \n",
       "2           News  https://www.nytimes.com/2018/04/24/dining/noma...  \n",
       "3           News  https://www.nytimes.com/2018/04/24/world/europ...  \n",
       "4           News  https://www.nytimes.com/2018/04/24/world/canad...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore the dataset using just one month of data\n",
    "\n",
    "April2018_headlines = pd.read_csv('/home/ubuntu/SynapseFI/headline_data/ArticlesApril2018.csv')\n",
    "April2018_headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8603"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load kaggle data\n",
    "\n",
    "curr_dir = '/home/ubuntu/SynapseFI/headline_data/'\n",
    "\n",
    "kaggle_headlines = []\n",
    "headline_counts = []\n",
    "\n",
    "for file in glob(curr_dir + '*.csv'):\n",
    "    article_df = pd.read_csv(file)\n",
    "    kaggle_headlines.extend(list(article_df.headline.values))\n",
    "    headline_counts.append(len(article_df.headline))\n",
    "\n",
    "kaggle_headlines = [h for h in kaggle_headlines if h != \"Unknown\"]\n",
    "len(kaggle_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of headlines in each month of Kaggle data: 1037.0\n"
     ]
    }
   ],
   "source": [
    "mean_headlines = round(np.mean(headline_counts))\n",
    "print('Mean number of headlines in each month of Kaggle data:', mean_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Augment the data obtained from Kaggle with more data using the New York Times API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The New York Times headlines data downloaded from Kaggle includes data from the months of January - May 2017 and January - April 2018 (and hence is missing data from May 2018). To compensate for this missing month of data, I downloaded data directly using the New York Times Archive API from May 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NYT_headlines(year, month):\n",
    "    \n",
    "    '''\n",
    "     This function makes a request to the New York Times Archive API and collects \n",
    "     a list of article headlines for the specified month and year of interest.\n",
    "    '''\n",
    "\n",
    "    api_key = {'api-key' : pickle.load(open('apikey.pkl','rb'))}\n",
    "    url = 'https://api.nytimes.com/svc/archive/v1/' + str(year) + '/' + str(month) + '.json'\n",
    "    \n",
    "    response = requests.get(url, params=api_key)\n",
    "    output = response.json()\n",
    "    \n",
    "    docs = output['response']['docs']\n",
    "    \n",
    "    headlines = []\n",
    "    for doc in docs:\n",
    "        headlines.append(doc['headline']['main'])\n",
    "    \n",
    "    return headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of headlines in the NYT May 2018 Archive: 7421\n"
     ]
    }
   ],
   "source": [
    "may_2018_headlines = get_NYT_headlines(2018, 5)\n",
    "may_2018_headlines[:10]\n",
    "print ('Number of headlines in the NYT May 2018 Archive:', len(may_2018_headlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more headlines for May 2018 from the NYT Archive than for a given month from the Kaggle dataset. As to not bias the training data from Kaggle's dataset towards the NYT Archive May 2018 data, I will randomly sample a quantity of May 2018 headlines equal to the number of headlines in an average month of the Kaggle dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Malaysia’s Election: What Happened, and What’s Next',\n",
       " 'Iraqi Election Front-Runner Moktada al-Sadr Courts Partners to Govern',\n",
       " 'Conjuring Spirits at the Tap Family Reunion',\n",
       " 'Palestinian Leader Apologizes After Speech Prompts Anti-Semitism Uproar',\n",
       " 'The Silence of the Bugs']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.random.choice(range(1,len(may_2018_headlines)), int(mean_headlines))\n",
    "may_2018_rand = [may_2018_headlines[i] for i in index]\n",
    "may_2018_rand[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to retrieve data from NYT Archive within a given date range\n",
    "\n",
    "def date_range_len(start_yr, start_mth, end_yr, end_mth):\n",
    "    '''\n",
    "    Calculates the total number of months within a date range of interest.\n",
    "    '''\n",
    "    months = (end_yr - start_yr)*12 + (end_mth - start_mth) + 1\n",
    "    return months\n",
    "\n",
    "def get_date_df(months, start_yr, start_mth):\n",
    "    '''\n",
    "    Creates a dataframe with columns date, year, and month for a given date range of interest. \n",
    "    Inputs include the total number of months within the date range, the start year, and the start month. \n",
    "    '''\n",
    "    yr_list = []\n",
    "    mth_list = []\n",
    "    date_list = []\n",
    "    curr_yr = start_yr\n",
    "    curr_mth = start_mth\n",
    "    for i in range(months):\n",
    "        yr_list.append(curr_yr)\n",
    "        mth_list.append(curr_mth)\n",
    "        date_list.append(dt.date(curr_yr, curr_mth, 1))\n",
    "        if curr_mth < 12:\n",
    "            curr_mth += 1\n",
    "        else:\n",
    "            curr_mth = 1\n",
    "            curr_yr += 1\n",
    "    date_df = pd.DataFrame(yr_list, columns=['year'])\n",
    "    date_df['month'] = mth_list\n",
    "    date_df['date'] = date_list\n",
    "    return date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download NYT Archive data from January 2019 through February 2019\n",
    "\n",
    "start_yr = 2019\n",
    "start_mth = 1\n",
    "end_yr = 2019\n",
    "end_mth = 2\n",
    "\n",
    "months = date_range_len(start_yr, start_mth, end_yr, end_mth)\n",
    "date_df = get_date_df(months, start_yr, start_mth)\n",
    "\n",
    "date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the get_NYT_headlines function to retrieve NYT Archive data for the months in date_df\n",
    "\n",
    "NYT_headlines = []\n",
    "for i, yr in enumerate(date_df['year']):\n",
    "#     print(yr, date_df['month'][i])\n",
    "    headlines = get_NYT_headlines(yr, date_df['month'][i])\n",
    "    NYT_headlines.extend(list(headlines))\n",
    "\n",
    "NYT_headlines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of headlines retrieved from NYT archive:\", len(NYT_headlines))\n",
    "\n",
    "# now we have a lot more data to train our model with \n",
    "# and can append this to the data downloaded from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge Kaggle data with NYT Archive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge Kaggle data with NYT Archive data\n",
    "\n",
    "all_headlines = kaggle_headlines + NYT_headlines\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've merged the headlines downloaded from Kaggle and the headlines downloaded using the NYT API together, we need to check for duplicates and remove those from our list of headlines we will use to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "\n",
    "len(all_headlines) == len(set(all_headlines))\n",
    "print(\"Number of duplicate headlines:\", len(all_headlines) - len(set(all_headlines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates in all_headlines\n",
    "\n",
    "seen = set()\n",
    "all_headlines_uniq = []\n",
    "for headline in all_headlines:\n",
    "    if headline not in seen:\n",
    "        all_headlines_uniq.append(headline)\n",
    "        seen.add(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# great, no more duplicate headlines\n",
    "\n",
    "len(all_headlines_uniq) == len(set(all_headlines_uniq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like some of the headlines in this list are simply empty strings. We will remove these from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty strings in all_headlines_uniq\n",
    "\n",
    "'' in all_headlines_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty strings\n",
    "\n",
    "all_headlines_uniq.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are also null values in the list of headlines. We will remove nulls as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values in all_headlines_uniq\n",
    "\n",
    "None in all_headlines_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null values\n",
    "\n",
    "all_headlines_uniq.remove(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of headlines in the training dataset:\", len(all_headlines_uniq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and tokenize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data consists of both cleaning and tokenizing the data. To clean the data, we will remove punctuation and lowercase all words in the corpus. We do not need to worry about removing stop words, as we do in many NLP projects, because we want to the model to generate fluid headlines similar to those that would be created by a human. \n",
    "\n",
    "We will clean and tokenize the corpus in the same step, using the Keras library's Tokenizer method. The tokenizer removes all punctuation including tabs and linebreaks, converts text to lowercase, and tokenizes/vectorizes all text in the corpus by turning each text into a sequence of integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sequence of n-gram tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've cleaned and tokenized the corpus, we will generate sequences of n-gram tokens. Language modelling takes in sequential data (i.e., words/tokens), and uses this sequential data to predict the next word/token. This requires tokenization, which is the process of extracting tokens (i.e., terms/words) from a corpus. To do this, I use the Python library Keras' built-in model for tokenization, which is able to obtain the tokens and their respsective index in the corpus. This allows for every text document in the dataset to be converted into a sequence of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each list in the above output represents the n-gram phrases generated from the input data (i.e., the documents/headlines). Each integer in these n-grams is the index of the given word in the complete vocabulary of words present in the corpus of text.\n",
    "\n",
    "For example, the first headline in the corpus is \"Finding an expansive view of a forgotten people in niger\". For this headline, we see the following output, i.e. **sequences of tokens**:\n",
    "\n",
    "    [169, 17],\n",
    "    [169, 17, 665],\n",
    "    [169, 17, 665, 367],\n",
    "    [169, 17, 665, 367, 4],\n",
    "    [169, 17, 665, 367, 4, 2],\n",
    "    [169, 17, 665, 367, 4, 2, 666],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170, 5],\n",
    "    [169, 17, 665, 367, 4, 2, 666, 170, 5, 667]\n",
    "\n",
    "which, respectively, equate to the following **n-grams**: \n",
    "\n",
    "    Finding an,\n",
    "    Finding an expansive,\n",
    "    Finding an expansive view,\n",
    "    Finding an expansive view of,\n",
    "    Finding an expansive view of a,\n",
    "    Finding an expansive view of a forgotten,\n",
    "    Finding an expansive view of a forgotten people,\n",
    "    Finding an expansive view of a forgotten people in,\n",
    "    Finding an expansive view of a forgotten people in niger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = all_headlines_uniq\n",
    "\n",
    "# max_words = 50000 # Max size of the dictionary (from Jeremy)\n",
    "\n",
    "# Clean and tokenize using Keras' built-in Tokenizer() method\n",
    "tokenizer = Tokenizer() # create the tokenizer\n",
    "tokenizer.fit_on_texts(corpus) # fit the tokenizer on the documents\n",
    "\n",
    "# convert data to sequence of tokens\n",
    "input_sequences = [] # sequences of n-grams from all documents in the corpus\n",
    "for headline in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([headline])[0] # list of tokens corresponding to each word in the document (i.e., news headline)\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1] # creates n-gram sequence (from 2 word n-grams, through n-grams = len(line))\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "print(\"Printing the first 5 input sequences...\")\n",
    "print(input_sequences[:5])\n",
    "\n",
    "nb_samples = sum(len(s) for s in input_sequences) # total number of samples in input_sequences; \n",
    "print(\"\\nTotal number of samples in input_sequences:\", nb_samples)\n",
    "\n",
    "# Vocab size\n",
    "V = len(tokenizer.word_index) + 1 # total words (WHY DO WE ADD 1??); word index gives a dictionary of words and their uniquely assigned integers\n",
    "print(\"\\nTotal number of words in the vocabulary:\", V)\n",
    "\n",
    "# Dimension to reduce to ## DELETE THIS?\n",
    "dim = 100\n",
    "window_size = 2\n",
    "# sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we've generated a data-set containing a sequence of tokens. It's important to recognize that different sequences may have different lengths. Before training the model, we must ensure all sequence lengths are equal. For this, we can pad the sequences using the Keras pad_sequence function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pad the sequences (CODE COMES FROM KAGGLE GUY)\n",
    "\n",
    "# find length of the longest input sequence (i.e., headline)\n",
    "max_sequence_len = max([len(s) for s in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')) # pads the input sequences and converts to a numpy array (must fit model on np arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Obtain variables (predictor and target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, inorder to input data and train the model, we must create predictors and a label. We will use a n-gram sequence as predictors, and the next word of the n-gram as the label. \n",
    "\n",
    "For example, for the headline \"Finding an expansive view of a forgotten people in niger\", the first three **predictors** will be:\n",
    "\n",
    "    1. Finding an\n",
    "    2. Finding an expansive\n",
    "    3. Finding an expansive view\n",
    "\n",
    "and the corresponding **labels** will be\n",
    "\n",
    "    1. expansive\n",
    "    2. view\n",
    "    3. of\n",
    "\n",
    "... and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1] # label is the last word of each n-gram, and predictors are all the preceeding words\n",
    "label = ku.to_categorical(label, num_classes=V) # one-hot encodes the labels; this is a matrix of 132,078 rows (the number of input sequences) and 20,387 columns (the number of total words/tokens in the word dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can obtain the input vector *X* and the label vector _Y_, which will be used to train the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Build and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a LSTM model with a three layer architecture, including:\n",
    "    \n",
    "***Input Layer*** : Takes the sequence of words as input\n",
    "<br /> ***LSTM Layer*** : Computes the output using LSTM units. There are currently 100 units, but this can be fine-tuned later.\n",
    "<br /> ***Dropout Layer*** : A regularization layer which randomly turns off the activations of some neurons in the LSTM layer in order to prevent overfitting. (Note: this is an optional layer)\n",
    "<br /> ***Output Layer*** : Computes the probability of the best possible next word as output\n",
    "    \n",
    "We will run this model for  100 epochs, but this can be further experimented with and fine-tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = max_sequence_len - 1 # length of the longest sequence - the length of the predictor (length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Keras Sequential model \n",
    "model = Sequential()\n",
    "\n",
    "## Specify the network architecture\n",
    "# Add Input Embedding Layer\n",
    "model.add(Embedding(V, 10, input_length=input_len)) ## replaced total_words with V to fix error -- is that correct??\n",
    "\n",
    "# Add Hidden Layer 1 - LSTM Layer\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Add Output Layer\n",
    "model.add(Dense(V, activation='softmax')) ## replaced total_words with V to fix error -- is that correct??\n",
    "\n",
    "# Compile the network (sets up training parameters before training)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the model architecture, we can train it using our data (predictors and labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the model\n",
    "\n",
    "model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Use the model to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been trained, we can use it to predict the next word based on input words (i.e., seed text). \n",
    "\n",
    "To feed this seed text into the model, we will need to do some pre-processing, which includes (1) tokenizing the seed text and (2) padding the sequences. We can then pass the sequences into the trained model to get the predicted word. The multiple predicted words can then be appended together to obtain a predicted sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] # list of tokens corresponding to each word in the document (i.e., news headline)\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # add padding\n",
    "        predicted = model.predict_classes(token_list, verbose=0) # make predictions using trained model\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items(): # word index gives a dictionary of words and their uniquely assigned integers\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"united states\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"president trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"donald trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"new york\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"science and technology\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (?)\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('\\n', loss_and_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement Ideas\n",
    "As we can see, the model has produced the output which looks fairly fine. The results can be improved further with following points:\n",
    "\n",
    "- Adding more data\n",
    "- Fine Tuning the network architecture\n",
    "- Fine Tuning the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
